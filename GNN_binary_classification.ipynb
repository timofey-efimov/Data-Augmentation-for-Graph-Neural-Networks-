{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Eitn1FobD_NG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "!pip install spektral"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#general libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os \n",
        "import glob \n",
        "import networkx as nx\n",
        "import pdb\n",
        "import networkx.algorithms.isomorphism as iso\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "#spektral\n",
        "from spektral.data import Graph\n",
        "from spektral.data import Dataset\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import TUDataset\n",
        "from spektral.models import GeneralGNN\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import QM9\n",
        "from spektral.layers import ECCConv, GlobalSumPool, GeneralConv, GatedGraphConv\n",
        "\n",
        "#tensorflow impors \n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import MeanAbsolutePercentageError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data import DisjointLoader\n",
        "from spektral.datasets import OGB\n",
        "from spektral.layers import ECCConv, GlobalSumPool"
      ],
      "metadata": {
        "id": "1c9F65hmGv-C"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n1, n2 = 10, 20 # 10 nodes\n",
        "m1, m2 = 20, 100  # 20 edges\n",
        "f1 = 100\n",
        "f2 = 100\n",
        "seed = 20160  # seed random number generators for reproducibility\n"
      ],
      "metadata": {
        "id": "u2h_uANFEPuz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spektral.data import Graph\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A dataset of random colored graphs.\n",
        "    The task is to classify each graph with the color which occurs the most in\n",
        "    its nodes.\n",
        "    The graphs have `n_colors` colors, of at least `n_min` and at most `n_max`\n",
        "    nodes connected with probability `p`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n, m, number, **kwargs):\n",
        "        self.n = n\n",
        "        self.m = m\n",
        "        self.number = number\n",
        "        self.seed = seed \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "        def make_graph(i):\n",
        "            G = nx.gnm_random_graph(self.n[i%2], self.m[i%2], seed=seed)\n",
        "            return Graph(x=nx.to_numpy_array(G), a=csr_matrix(nx.to_numpy_array(G)), y=i)\n",
        "\n",
        "        # We must return a list of Graph objects\n",
        "        print(self.number)\n",
        "        return [make_graph(_) for _ in range(2*self.number)]\n"
      ],
      "metadata": {
        "id": "wv9x3oCSFl4c"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = MyDataset(n=[10,20], m=[20,100], number=100)\n",
        "data_test = MyDataset(n=[10,20], m=[20,100], number=200)\n",
        "data_val = MyDataset(n=[10,20], m=[20,100], number=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OczXOBUwKd2p",
        "outputId": "d95938f9-aca2-44f3-c0e3-2521cb0002f6"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "200\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_array = []\n",
        "training_loss = []"
      ],
      "metadata": {
        "id": "dCdfgXnZMHFr"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################################################################\n",
        "# Config\n",
        "################################################################################\n",
        "learning_rate = 1e-3\n",
        "epochs = 250\n",
        "batch_size = 32\n",
        "\n",
        "################################################################################\n",
        "# Load data\n",
        "################################################################################\n",
        "dataset = data_train  # Set amount=None to train on whole dataset\n",
        "\n",
        "# Parameters\n",
        "F = dataset.n_node_features  # Dimension of node features\n",
        "#S = dataset.n_edge_features  # Dimension of edge features\n",
        "n_out = dataset.n_labels  # Dimension of the target\n",
        "\n",
        "# Train/test split\n",
        "idxs = np.random.permutation(len(dataset))\n",
        "split = int(0.9 * len(dataset))\n",
        "idx_tr, idx_te = np.split(idxs, [split])\n",
        "dataset_tr, dataset_te = data_train+data_val, data_test\n",
        "# dataset_val = data_val\n",
        "\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_val = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=1)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=100)\n",
        "\n",
        "################################################################################\n",
        "# Build model\n",
        "dp_rate = 0.7\n",
        "################################################################################\n",
        "class Net(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv0 = GeneralConv(channels = 64, n_layers = 2, activation=\"relu\", dropout=dp_rate)\n",
        "        self.conv1 = GeneralConv(channels = 128, n_layers = 2, activation=\"relu\", dropout=dp_rate)\n",
        "        # self.conv2 = GeneralConv(128, activation=\"relu\", dropout=dp_rate)\n",
        "        # self.conv3 = GeneralConv(64, activation=\"relu\", dropout=dp_rate)\n",
        "        self.conv4 = GeneralConv(channels = 64, n_layers = 2,activation=\"relu\", dropout=dp_rate)\n",
        "        self.global_pool = GlobalSumPool()\n",
        "        self.dense = Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, a, e = inputs\n",
        "        x = self.conv0([x, a])\n",
        "        x = self.conv1([x, a])\n",
        "        # x = self.conv2([x, a])\n",
        "        # x = self.conv3([x, a])\n",
        "        x = self.conv4([x, a])\n",
        "        output = self.global_pool(x)\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "model = Net()\n",
        "optimizer = Adam(learning_rate)\n",
        "loss_fn1 = BinaryCrossentropy()\n",
        "loss_fn2 = BinaryCrossentropy()\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# Fit model\n",
        "################################################################################\n",
        "@tf.function(input_signature=loader_tr.tf_signature(), experimental_relax_shapes=True)\n",
        "def train_step(inputs, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn1(target, predictions) + sum(model.losses)\n",
        "    # loss_val = 0\n",
        "    # for batch_val in loader_val:\n",
        "    #     inputs_val, target_val = batch_val\n",
        "    #     predictions_val = model(inputs_val, training=False)\n",
        "    #     loss_val += loss_fn(target_val, predictions_val)\n",
        "    # loss_val /= loader_val.steps_per_epoch\n",
        "    # val_loss_array.append(loss_val)\n",
        "    # print(\"Validation Loss is: {}\".format(loss_val))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "\n",
        "step = loss = 0\n",
        "for batch in loader_tr:\n",
        "    step += 1\n",
        "    loss += train_step(*batch)\n",
        "    if step == loader_tr.steps_per_epoch:\n",
        "        step = 0\n",
        "        print(\"Loss: {}\".format(loss / loader_tr.steps_per_epoch))\n",
        "        training_loss.append(format(loss / loader_tr.steps_per_epoch))\n",
        "        loss = 0\n",
        "\n",
        "################################################################################\n",
        "# Evaluate model\n",
        "################################################################################\n",
        "print(\"Testing model\")\n",
        "loss = 0\n",
        "for batch in loader_te:\n",
        "    inputs, target = batch\n",
        "    predictions = model(inputs, training=False)\n",
        "    loss += loss_fn2(target, predictions)\n",
        "loss /= 100*loader_te.steps_per_epoch\n",
        "print(\"Done. Test loss: {}\".format(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "BKIfrNtnGan5",
        "outputId": "50846754-5eba-47b3-d57f-d26ef9be6560"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spektral/data/utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
            "  np.random.shuffle(a)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-91ee650ae0df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_tr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spektral/data/loaders.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mnxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spektral/data/loaders.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollate_labels_disjoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_disjoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp_matrices_to_sp_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spektral/data/utils.py\u001b[0m in \u001b[0;36mto_disjoint\u001b[0;34m(x_list, a_list, e_list)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mx_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Adjacency matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 10 and the array at index 7 has size 20"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loader_tr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um5S53chMjxc",
        "outputId": "b7eb95fa-53a4-4cfc-b4e9-a6ea70e62035"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<spektral.data.loaders.DisjointLoader object at 0x7f0470c84bd0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gYZlPlG3McpD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}